p8105\_hw3\_sg3891
================
Sandya Ganesh
2021-10-20

## Question 1

We will explore the instacart data set

#### Short description of instacart data set

``` r
data("instacart")
```

The code below provides the size and structure of the data, describes
some key variables, and gives illustrative examples of observations.

The total number of observations in the instacart data set is 1384617
observations with 15 variables. Some key variables in this data set
include order\_id, user\_id, product\_name. There are 131209 unique
orders in this data set, with 131209 users placing these orders, which
means that each user has placed 1 instacart order in this dataset. These
orders include products such as Bulgarian Yogurt, Organic 4% Milk Fat
Whole Milk Cottage Cheese, Organic Celery Hearts. This dataset includes
orders for 39123 unique products.

There are 134 aisles, and the aisles that the most items are ordered
from are packaged vegetables fruits, fresh fruits, fresh vegetables.

#### Question 1.1

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
aisle_df = 
  instacart %>% 
  count(aisle) %>% 
  filter(n > 10000)

ggplot(aisle_df, aes(x = reorder(aisle, -n), y = n)) + 
  geom_point() + 
  labs(
    title = "Instacart Popular Aisles ",
    x = "Aisle Names",
    y = "Number of items ordered",
    caption = "Data from the instacart dataset"
  ) +
  coord_flip()
```

![](p8105_hw3_sg3891_files/figure-gfm/q1_plot-1.png)<!-- -->

#### Question 1.2

Below is a table showing the three most popular items in each of the
aisles “baking ingredients”, “dog food care”, and “packaged vegetables
fruits”, with the number of times each item is ordered.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  rename(item_count = n) %>% 
  mutate(order_count = min_rank(desc(item_count))) %>% 
  filter(order_count < 4) %>% 
  arrange(aisle, order_count) %>% 
  select(aisle, product_name, item_count) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 | item\_count |
|:---------------------------|:----------------------------------------------|------------:|
| baking ingredients         | Light Brown Sugar                             |         499 |
| baking ingredients         | Pure Baking Soda                              |         387 |
| baking ingredients         | Cane Sugar                                    |         336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |          30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |          28 |
| dog food care              | Small Dog Biscuits                            |          26 |
| packaged vegetables fruits | Organic Baby Spinach                          |        9784 |
| packaged vegetables fruits | Organic Raspberries                           |        5546 |
| packaged vegetables fruits | Organic Blueberries                           |        4966 |

#### Question 1.3

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name =="Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(average = round(mean(order_hour_of_day), 2)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = average
  ) %>%
   rename( "Sunday" = "0","Monday" = "1", "Tuesday" = "2", "Wednesday" = "3", "Thursday" = "4", "Friday" = "5", "Saturday" = "6") %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the `.groups` argument.

| product\_name    | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |  13.77 |  14.32 |   15.38 |     15.32 |    15.22 |  12.26 |    13.83 |
| Pink Lady Apples |  13.44 |  11.36 |   11.70 |     14.25 |    11.55 |  12.78 |    11.94 |

## Question 2

The following question uses the BRFSS data.

#### Clean the BRFSS data

The code chunk below cleans the data:

``` r
data("brfss_smart2010")

brfss_cleaned = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic %in% c("Overall Health")) %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>%
  arrange(response) %>%
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%
  separate(locationdesc, into = c('state', 'location'), sep = ' - ')
```

#### Question 2.1

In 2002, which states were observed at 7 or more locations?

``` r
brfss_cleaned %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  distinct(location) %>% 
  count(state) %>% 
  filter(n >= 7) %>% 
  select(state)
```

    ## # A tibble: 6 × 1
    ## # Groups:   state [6]
    ##   state
    ##   <chr>
    ## 1 CT   
    ## 2 FL   
    ## 3 MA   
    ## 4 NC   
    ## 5 NJ   
    ## 6 PA

As seen above, the states observed at 7 or more locations include
Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and
Pennsylvania.

In 2010, which states were observed at 7 or more locations?

``` r
brfss_cleaned %>% 
  filter(year == "2010") %>% 
  group_by(state) %>% 
  distinct(location) %>% 
  count(state) %>% 
  filter(n >= 7) %>% 
  select(state)
```

    ## # A tibble: 14 × 1
    ## # Groups:   state [14]
    ##    state
    ##    <chr>
    ##  1 CA   
    ##  2 CO   
    ##  3 FL   
    ##  4 MA   
    ##  5 MD   
    ##  6 NC   
    ##  7 NE   
    ##  8 NJ   
    ##  9 NY   
    ## 10 OH   
    ## 11 PA   
    ## 12 SC   
    ## 13 TX   
    ## 14 WA

As seen above, the states observed at 7 or more locations include
California, Colorado, Florida, Massachusetts, Maryland, North Carolina,
Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina,
Texas, and Washington.

#### Question 2.2

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state.

Additionally, make a “spaghetti” plot of this average value over time
within a state (that is, make a plot showing a line for each state
across years – the geom\_line geometry and group aesthetic will help).

``` r
spaghetti_df = brfss_cleaned %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(average = round(mean(data_value, na.rm = TRUE), 2)) %>%
  select(year, state, average)
```

    ## `summarise()` has grouped output by 'state'. You can override using the `.groups` argument.

``` r
ggplot(spaghetti_df, aes(x = year, y = average, color = state)) + 
  geom_line() + 
  labs(
    title = "Average value over time for all states ",
    x = "Time (years)",
    y = "Average value",
    caption = "Data from the brfss dataset"
  )
```

![](p8105_hw3_sg3891_files/figure-gfm/q2_spaghetti-1.png)<!-- -->
\#\#\#\# Question 2.3 Make a two-panel plot showing, for the years 2006,
and 2010, distribution of data\_value for responses (“Poor” to
“Excellent”) among locations in NY State.

``` r
two_2006_2010 = brfss_cleaned %>% 
  filter(year == "2006" | year == "2010") %>% 
  filter(state == "NY")

ggplot(two_2006_2010, aes(x = data_value, color = response)) + 
  geom_density() + 
  facet_grid(. ~ year) +
  labs(
    title = "Distribution of data_value for responses in NY",
    x = "Response",
    y = "data_value",
    caption = "Data from the brfss dataset"
  )
```

![](p8105_hw3_sg3891_files/figure-gfm/q2_panels-1.png)<!-- -->

## Question 3

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes.

#### Load and clean the accel data

``` r
accel_cleaned = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  mutate(
    day_type = ifelse(day == "Saturday" | day == "Sunday", "Weekend", "Weekday"),
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
    ) %>%
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity_counts",
    names_prefix = "activity.",
  )
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Describe the resulting dataset (e.g. what variables exist, how many
observations, etc)

Looking into this data set, we can see that the total number of
observations in the accel data set is 50400 observations with 6
variables. The variables in this data set include order\_id,
product\_id, add\_to\_cart\_order, reordered, user\_id, eval\_set.

#### Question 3.1

Using your tidied dataset, aggregate across minutes to create a total
activity variable for each day, and create a table showing these totals.
Are any trends apparent?

``` r
accel_total_daily = accel_cleaned %>% 
  group_by(day_id) %>% 
  summarize(total_daily_activity = sum(activity_counts)) %>%
  select(day_id, total_daily_activity) %>% 
  knitr::kable()

accel_total_daily
```

| day\_id | total\_daily\_activity |
|--------:|-----------------------:|
|       1 |              480542.62 |
|       2 |               78828.07 |
|       3 |              376254.00 |
|       4 |              631105.00 |
|       5 |              355923.64 |
|       6 |              307094.24 |
|       7 |              340115.01 |
|       8 |              568839.00 |
|       9 |              295431.00 |
|      10 |              607175.00 |
|      11 |              422018.00 |
|      12 |              474048.00 |
|      13 |              423245.00 |
|      14 |              440962.00 |
|      15 |              467420.00 |
|      16 |              685910.00 |
|      17 |              382928.00 |
|      18 |              467052.00 |
|      19 |              371230.00 |
|      20 |              381507.00 |
|      21 |              468869.00 |
|      22 |              154049.00 |
|      23 |              409450.00 |
|      24 |                1440.00 |
|      25 |              260617.00 |
|      26 |              340291.00 |
|      27 |              319568.00 |
|      28 |              434460.00 |
|      29 |              620860.00 |
|      30 |              389080.00 |
|      31 |                1440.00 |
|      32 |              138421.00 |
|      33 |              549658.00 |
|      34 |              367824.00 |
|      35 |              445366.00 |

Looking at the above table, it appears that the subject had very little
activity on day 24 and 31. Other than that, activity appears to
fluctuate throughout the 35 days, with no clear trend.

#### Question 3.2

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week. Describe in
words any patterns or conclusions you can make based on this graph.
